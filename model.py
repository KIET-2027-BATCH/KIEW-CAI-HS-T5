import os
import pandas as pd
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report

# ‚úÖ Download required NLTK data
nltk.download('stopwords')
nltk.download('wordnet')

# ‚úÖ Initialize text processing tools
lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))

# ‚úÖ Text Preprocessing Function
def preprocess_text(text):
    if isinstance(text, str):  
        text = text.lower()  # Convert to lowercase
        text = re.sub(r'[^a-zA-Z\s]', '', text)  # Remove punctuation
        words = text.split()  
        words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]  
        
        # Ensure empty text is replaced with a default value
        return ' '.join(words) if words else "emptytext"
    
    return "emptytext"

# ‚úÖ Load Dataset
file_path = r"C:\Users\R HARIKA\Downloads\PRODUCT REVIEWS\Amazon-Product-Reviews - Amazon Product Review (1).csv"
if not os.path.exists(file_path):
    print("‚ùå Error: Dataset file not found. Check file path.")
    exit()

df = pd.read_csv(file_path, encoding='utf-8')

# ‚úÖ Check dataset contents
if df.empty:
    print("‚ùå Error: The dataset is empty. Ensure correct file is used.")
    exit()

# ‚úÖ Drop missing values
df.dropna(subset=['review_body', 'sentiment'], inplace=True)

# ‚úÖ Apply Preprocessing
df['cleaned_text'] = df['review_body'].astype(str).apply(preprocess_text)

# ‚úÖ Debugging: Check dataset after preprocessing
print("\nüìå First 5 cleaned reviews:")
print(df[['review_body', 'cleaned_text']].head())

# ‚úÖ Ensure dataset is not empty after preprocessing
if df['cleaned_text'].str.strip().eq('emptytext').all():
    print("‚ùå Error: All reviews are empty after preprocessing. Adjust preprocessing function.")
    exit()

# ‚úÖ Handle dataset imbalance
df['sentiment'] = df['sentiment'].astype(str)  
min_count = df['sentiment'].value_counts().min()  

if min_count < 5:
    print("‚ùå Error: Some sentiment classes have very few samples. Consider removing them or collecting more data.")
    exit()

df_balanced = df.groupby('sentiment', group_keys=False).apply(lambda x: x.sample(n=min_count, random_state=42))

# ‚úÖ Ensure balanced dataset is not empty
if df_balanced.empty:
    print("‚ùå Error: Dataset is empty after balancing. Check sentiment distribution.")
    exit()

# ‚úÖ Vectorization using TF-IDF
vectorizer = TfidfVectorizer(max_features=5000)

try:
    X = vectorizer.fit_transform(df_balanced['cleaned_text']).toarray()
    y = df_balanced['sentiment'].values
except ValueError as e:
    print(f"‚ùå TF-IDF Error: {e}")
    exit()

# ‚úÖ Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# ‚úÖ Train Model
model = LogisticRegression()
model.fit(X_train, y_train)

# ‚úÖ Evaluate Model
y_pred = model.predict(X_test)
print("\nüîπ Model Accuracy:", accuracy_score(y_test, y_pred))
print("\nüîπ Classification Report:\n", classification_report(y_test, y_pred))

# ‚úÖ User Input Prediction
def predict_sentiment():
    user_input = input("\nEnter a product review: ")
    cleaned_input = preprocess_text(user_input)  
    input_vector = vectorizer.transform([cleaned_input]).toarray()  
    prediction = model.predict(input_vector)[0]  
    
    # ‚úÖ Convert 1 ‚Üí Positive, 0 ‚Üí Negative in output
    sentiment_label = "Positive" if prediction == "1" else "Negative"
    print(f"\nPredicted Sentiment: {sentiment_label} ({prediction})")

predict_sentiment()
import joblib

# Save Model
joblib.dump(model, "sentiment_model.pkl")

# Save Vectorizer
joblib.dump(vectorizer, "tfidf_vectorizer.pkl")